注：此文档暂时只记录了线性拟合，非线性暂未涉及

**插值和拟合的区别：**
插值算法中，得到的多项式f(x)要经过所有样本点。但是如果样本点太多，那么这个多项式次数过高，会造成龙格现象。  
尽管我们可以选择分段的方法避免这种现象，但是更多时候我们更倾向于得到一个确定的曲线，尽管这条曲线不能经过每一个样本点，但只要*保证误差足够小*即可，这就是拟合的思想。 (拟合的结果是得到一个确定的曲线）

**拟合的大致两种思路（来源知乎）：**
1. 一种是看着像啥样或基于先验知识给出常见函数的关系式，通过数据拟合得到相应的系数；
2. 第二种是直接从数据出发，采用“基函数”拟合，和泰勒展开、级数有关系，函数越复杂拟合的越完美，但**泛化**能力就有待考究了，即复杂度越高越容易出现[过拟合](https://www.zhihu.com/search?q=%E8%BF%87%E6%8B%9F%E5%90%88&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22471507773%22%7D)

## 最小二乘拟合
已知一组二维数据，即平面上的n个点$(x_i,y_i)(i=1,2,...,n)$，要寻求一个一个函数（曲线）$y=f(x)$，使$f(x)$在某种准则下与所有数据点最为接近，即曲线拟合得最好。记$$\delta_i=f(x_i)-y_i,\ i=1,2,...,n$$称为拟合函数$f(x_i)$在$x_i$点处的残差。

**最小二乘准则**：为使$f(x)$在整体上尽可能与给定数据最为接近，以“残差的平方和最小”为判定准则，即令$$L=\sum_{i=1}^n(f(x_i)-y_i)^2$$
达到最小值

注：
1. 为什么不用四次方？  
（1）避免极端数据对拟合曲线的影响。  
（2）最小二乘法得到的结果和MLE极大似然估计一致。  
2. 不用奇数次方的原因：误差会正负相抵

### 线性最小二乘
注：清风的求解过程主要使用*导数*方法，但计算可能会较为复杂，使用*线性代数*的关键公式$A^TAX =A^TB$ 可以较简单地解决， 此方法在MIT线代课中有一定的介绍[麻省理工学院 - MIT - 线性代数_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16Z4y1U7oU/?spm_id_from=333.337.search-card.all.click&vd_source=ff081f9d2fd264d1cecbbcb4994bd540)

给定一个线性无关的函数系$\{\varphi_k(x) | k=1,2,...,m\}$,如果拟合函数以其*线性组合*的形式$$f(x)=\sum_{k=1}^m a_k\varphi_k(x)$$出现(参数仅以*一次形式*出现，且*不能乘以或除以其他任何的参数*，并*不能出现参数的复合函数*形式），例如$f(x)=a_1x^{m-1}+a_2x_{m-2}+...+a_{m-1}x+a_m$或者$f(x)=\sum_{k=1}^ma_kcos(kx)$，则$f(x)=f(x,a_1,a_2,...,a_m)$就是关于参数$a_1,a_2,...,a_m$的线性函数
![[微信截图_20230718180925.png]]
将上式代入误差计算函数，则目标函数$J=J(a_1,a_2,...,a_k)$是关于参数$a_1,a_2,...,a_m$的多元函数。
由$$\frac{\partial L}{\partial a_k}=0,\ k=1,2,...m$$
亦即$\sum_{i=1}^n[(f(x_i)-y_i) \varphi_k(x_i)]=0, \ k=1,2,...,m$
可得$$\sum_{j=1}^m[\sum_{i=1}^n \varphi_j(x_i) \varphi_k(x_i)]a_j=\sum_{i=1}^n y_i\varphi_k(x_i), \ k=1,2,...,m$$
于是上式形成了一个关于$a_1,a_2,...,a_m$的线性方程组，称为*正规方程组*$$R=\begin{equation}\begin{bmatrix}
\varphi_1(x_1) & \varphi_2(x_1)& \cdots & \varphi_m(x_1) \\
\varphi_1(x_2) & \varphi_2(x_2)& \cdots & \varphi_m(x_2) \\
\vdots & \vdots &\ddots & \vdots \\ 
\varphi_1(x_n) & \varphi_2(x_n) &\cdots & \varphi_m(x_n) \end{bmatrix}\end{equation}$$
$$A=\begin{equation}\begin{bmatrix} a_1\\ a_2 
\\ \vdots \\ a_m \end{bmatrix}\end{equation},$$
$$Y=\begin{equation}\begin{bmatrix} y_1\\ y_2 
\\ \vdots \\ y_n \end{bmatrix}\end{equation}$$
则正规方程组式可表示为$R^TRA=R^TY$
由代数知识可知，当矩阵R是**列满秩**时，$R^TR$ 是**可逆**的，于是正规方程组有唯一解，即$$A=(R^TR)^{-1}R^TY$$为所求的拟合函数的系数，就可得到最小二乘拟合系数$f(x)$

#### 最简单的形式:y=kx+b
设拟合曲线为$y=kx+b$，问 $k$ 和 $b$ 取何值时，样本点与拟合曲线*最接近*。

此时令拟合值记为$\hat y_i = kx_i+b$，则要找到 $\hat k, \hat b = arg\min_{k,b}(\sum_{i=1}^n(y_i-\hat y_i)^2)=arg\min_{k,b}(\sum_{i=1}^n(y_i-kx_i-b)^2)$，即使得残差最小的k, b值，得到$$L=\sum_{i=1}^n(y_i-kx_i-b)^2$$最小(L在机器学习中被称为损失函数，在回归中称为残差平方和)，在本节课中清风采取的是求偏导，偏导为0时即对应的残差最小，此时得到的k,b为所求值
![[微信截图_20230718194803.png]]

在后续的GM灰色预测模型时，补充了关于最小二乘法(OLS)的矩阵计算，其大体思路跟上述一致，即把 $L$ 用矩阵乘法表示
令$$L=\begin{bmatrix}
y_1-kx_1-b,\ y_2-kx_2-b,\ ..., \ y_n-kx_n-b\ \end{bmatrix}\begin{bmatrix}
y_1-kx_1-b\\ y_2-kx_2-b\\ ... \\ y_n-kx_n-b\ \end{bmatrix}$$
令矩阵 $Y=\begin{bmatrix} y_1\\y_2\\...\\y_n \end{bmatrix},\quad  X=\begin{bmatrix} 1 &x_1\\1 &x_2\\ \vdots &\vdots \\1 &x_n \end{bmatrix}(相当于上面的R矩阵)， \quad \beta=\begin{bmatrix} b\\ k \end{bmatrix}(相当于上面的A矩阵)$
则$X\beta = \begin{bmatrix} b+kx_1\\b+kx_2\\...\\b+kx_1 \end{bmatrix},\quad Y-X\beta =\begin{bmatrix} y_1-kx_1-b\\y_2-kx_2-b\\...\\y_n-kx_n-b \end{bmatrix}$

所以有$L=(Y-X\beta)^T(Y-X\beta)=Y^TY-Y^TX\beta-\beta^TX^T\beta^TX^T+\beta^TX^TX\beta, \quad \frac{\partial L}{\partial \beta}=-X^TY-X^TY+2X^TX\beta=0$，推得$X^TX\beta=X^TY$，所以$\hat \beta = (X^TX)^{-1}X^TY$

**补充：**
![[微信截图_20230726101127.png]]

##### 例题 建立刀片磨损的经验公式

| $t_i$ | 0 | 1   | 2   | 3   |4   | 5   | 6   | 7 |
| :-----: | :---: | :---: | :---: | :---: | :---: | :---: | :--: | :---: |
| $y_i$   | 27.0     | 26.8   | 26.5   | 26.3   | 26.1   | 25.7   | 25.3 | 24.8   |

解：
**方法一：**（偏导方法）
拟合参数$a,b$ 的准则是最小二乘准则，即求$a,b$，使得$\hat a, \hat b = arg\min_{k,b}(\sum_{i=1}^8(y_i-\hat y_i)^2)=arg\min_{k,b}(\sum_{i=1}^8(y_i-kx_i-b)^2)$

由极值的必要条件，得
$$\begin{cases} \frac{\partial \delta}{\partial a}=2\sum_{i=1}^8(at_i+b-y_i)t_i=0,\\ \frac{\partial \delta}{\partial b}=2\sum_{i=1}^8(at_i+b-y_i)=0 \\  \end{cases}$$
化简，得到正规方程组$$\begin{cases}a \sum_{i=1}^8 t_i^2 + b\sum_{i=1}^8t_i=\sum_{i=1}^8 y_it_i,\\ a\sum_{i=1}^8 t_i + 8b = \sum_{i=1}^8 y_i\end{cases}$$
解得$a,b$的估计值分别为$$\begin{cases}\hat a = \frac{\sum_{i=1}^8 (t_i-\bar t)(y_i - \bar y)}{\sum_{i=1}^8 (t_i - \bar t)^2}\\ \hat b=\bar y-\hat a \bar t \end{cases}$$
式中，$\bar t = \frac{1}{8} \sum_{i=1}^8 t_i, \bar y = \frac{1}{8} \sum_{i=1}^8 y_i$分别为 $t_i和y_i$ 的均值

**方法二：**（线代方法）
直接计算$$A = (R^TR)^{-1} R^TY$$
**代码实现：**
```python
import numpy as np  # 导入 numpy 模块，用于科学计算  
t = np.arange(8)  # 生成一个从 0 到 7 的整数数组，作为自变量  
y = np.array([27.0, 26.8, 26.5, 26.3, 26.1, 25.7, 25.3, 24.8])  # 定义一个数组，作为因变量  
  
# 方法一  
tb = t.mean()  
yb = y.mean()  # 计算自变量和因变量的均值  
  
a1 = sum((t-tb)*(y-yb))/sum((t-tb)**2)  # 根据最小二乘法的公式，计算拟合直线的斜率  
b1 = yb-a1*tb  # 根据最小二乘法的公式，计算拟合直线的截距  
print('拟合的多项式系数：', [a1, b1])  # 输出第一种方法的解，即拟合直线的系数  
# 方法二  
A = np.vstack([t, np.ones(len(t))]).T  # 构造一个矩阵 A，其第一列是自变量 t，第二列是全 1
p = np.linalg.pinv(A.T @ A) @ A.T @ y  # 利用矩阵运算和伪逆求解线性方程组，得到拟合直线的系数  
# p=np.linalg.pinv(A) @ y # 另一种写法，等价于上面的式子  
print('拟合的多项式系数：', p)   # 输出第二种方法的解，与第一种方法相同
```

注：`p = np.linalg.pinv(A.T @ A) @ A.T @ y` 和 `p=np.linalg.pinv(A) @ y` 得到相同的结果是因为 $A$ 的伪逆定义为 $(A^T A)^{-1} A^T$ 正好和最小二乘法的要求匹配。且`np.linalg.pinv`函数会自动计算伪逆，所以二者等价


### 多项式拟合
`Python`多项式拟合的函数为`polyfit`，调用格式为
```python
p = polyfit(x,y,n) # 拟合n次多项式，返回值p是多项式对应的系数，排列次序为从高次幂系数到低次幂系数
```

举例:
```python
import numpy as np  
import matplotlib as mpl  
import matplotlib.pyplot as plt  
from numpy import polyfit, poly1d  
  
x = np.linspace(-5, 5, 100) # 产生[-5,5]的100个等间隔的数组  
y = 4 * x + 1.5 # y是关于x的一次函数  
noise_y = y + np.random.randn(y.shape[-1]) * 2.5  # y添加噪声后的函数值。  
print(y.shape[-1]) # 100个元素  
  
# p = plt.plot(x, noise_y, 'rx') # 画红色叉叉就是rx，画红色叉叉虚线图就是rx--  
# p = plt.plot(x, y, 'b:') # 画蓝色点图，"b" 表示蓝色，":"表示点图。  
  
coeff = polyfit(x, noise_y, 1)  # 计算系数  
print(coeff)  
  
p = plt.plot(x, noise_y, 'rx')  
p = plt.plot(x, coeff[0] * x + coeff[1], 'k-') # 画黑色实线图，"k" 表示实线，"-"表示实线。  
p = plt.plot(x, y, 'b--') # 这里可看出拟合出一阶函数与原函数重合了，通过注释该语句看出。  
  
# 还可以用 poly1d 生成一个以传入的 coeff 为参数的多项式函数：  
# f = poly1d(coeff)  
# p = plt.plot(x, noise_y, 'rx') # 绘制红色叉叉散点图  
# p = plt.plot(x, f(x)) # 带入x点，画出f函数。  
  
plt.show()
```
![[微信截图_20230724104856.png]]

计算多项式p在x处的函数命令为
```python
y = polyval(p,x)
```

#### 例题：根据下面反应物的观测值数据定出经验公式$y=ke^{mt}$

| $t_i$ | 3 | 6   | 9   | 12   | 15  | 18   | 21   | 24 |
| :-----: | :---: | :---: | :---: | :---: | :---: | :---: | :--: | :---: |
| $y_i$   | 57.6 | 41.9 | 31.0 | 22.7 | 16.6  | 12.2 | 8.9 | 6.5 |

**注**：这个经验公式原本是非线性的(复合了$k$ 和$m$)，但可以通过某些代换使得代换后的函数是线性的，进而求解

解：对$y=ke^{mt}$两边取对数，得到$\ln y=\ln k+mt$, 记$\ln k=b$ ，则有$\ln y=b+mt$，使用线性最小法拟合参数$b, m$, 即求$b,m$的估计值使得$$\sum_{i=1}^8(b+mt_i-\ln y_i)^2$$达到最小值

#### 代码实现
```python
# 导入numpy模块，用于进行数值计算  
import numpy as np  
  
# 从data7_13.txt文件中读取数据，存储在a变量中  
a = np.loadtxt('data7_13.txt')  
# 将a的第一列赋值给t，表示时间  
t = a[0]  
# 将a的第二列赋值给y，表示反应物的量  
y = a[1]  
# 对y取对数，为了进行线性拟合，np.log默认以e为底  
y = np.log(y)  
# 使用np.polyfit函数对t和y进行1次多项式拟合，返回拟合系数p  
p = np.polyfit(t, y, 1)  # 拟合1次多项式，即ax+b这种形式
# 打印拟合系数p  
print("多项式的系数为：", p)  
# 计算并打印m的值，m是p的第一个元素，表示斜率  
print("m=", round(p[0], 4))  
# 计算并打印k的值，k是e的p[1]次方，表示截距,保留4位数字  
print("k=", round(np.exp(p[1]), 4))
```

### 非线性拟合的python实现

对于上面的例子，也可以使用`curve_fit`函数实现
```python
from scipy.optimize import curve_fit
popt,pcov = curve_fit(func,xdata,ydata) 
```
参数说明：
- `func`：拟合的函数
- `xdata`：自变量的观测值
- `ydata`：函数的观测值
返回值：
- `popt`：拟合参数
- `pcov`：参数的协方差矩阵

**代码实现（用curve_fit函数拟合上面的例子，并求t=5.8时的预测值）：**
```python
import numpy as np # 导入numpy模块，用于处理数组和矩阵运算  
from scipy.optimize import curve_fit # 导入scipy.optimize模块中的curve_fit函数，用于曲线拟合  
  
def y(t, k, m): # 定义一个函数y，接受三个参数t, k, m，并返回k*np.exp(m*t)  
    return k*np.exp(m*t)  
  
a = np.loadtxt('data7_13.txt')# 从data7_13.txt文件中读取数据，存储为numpy数组a 
t0 = a[0]; y0 = a[1]  # 将a的第一行赋值给t0，表示自变量；将a的第二行赋值给y0，表示因变量  
# y = lambda t, k, m: k*np.exp(m*t) # 这一行是用lambda表达式定义的函数y，与上面的def定义的函数y等价  
p, pcov = curve_fit(y, t0, y0)  # 调用curve_fit函数，传入函数y和数据t0, y0，返回拟合的参数p和协方差矩阵pcov  
print('拟合的参数为：', np.round(p, 4))  # 打印拟合的参数p，按照当时函数给的参数顺序打印，并保留四位小数  
yh = y(np.array([5, 8]), *p)  # 调用函数y，传入一个数组[5, 8]和拟合的参数p，返回预测值yh  
print('预测值为：', np.round(yh, 4))  # 打印预测值yh，并保留四位小数
```

关于此函数的其他示例：
```python
import matplotlib.pyplot as plt 
import numpy as np from scipy.optimize 
import curve_fit 

# 自定义函数 
def func(x, a, b, c): 
	return a * np.exp(-b * x) + c 

# 构造数据 
xdata = np.linspace(0, 4, 50) 
y = func(xdata, 2.5, 1.3, 0.5) 
rng = np.random.default_rng() 
y_noise = 0.2 * rng.normal(size=xdata.size) 
ydata = y + y_noise 

# 拟合 
popt, pcov = curve_fit(func, xdata, ydata) 

# 设置参数取值范围 
popt1, pcov1 = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5])) 

# 可视化 
plt.plot(xdata, ydata, 'b-', label='data') 
plt.plot(xdata, func(xdata, *popt), 'r-', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt)) 
plt.plot(xdata, func(xdata, *popt1), 'g--', label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt1)) 

plt.legend() 
plt.show()
```
![[微信截图_20230724103328.png]]

### 检验参数解释
#### 误差平方和SSE
计算拟合数据与原始数据对应点的误差平方和，$$SSE=\sum_{i=1}^n(y_i-\hat y_i)^2$$
#### 总体平方和SST
$$SST=\sum_{i=1}^n(y_i-\bar y)^2$$其中，$SST=SSE+SSR$
![[微信截图_20230718215859.png]]

#### 回归平方和SSR
反映自变量对y的影响
$$SSR=\sum_{i=1}^n(\hat y_i-\bar y)^2$$
#### 拟合优度（可决系数）$R^2$
$$0\leq R^2=\frac{SSR}{SST}=\frac{SST-SSe}{SST}=1-\frac{SSE}{SST}\leq 1$$
$R^2$越接近1，说明误差平方和越接近0，误差越小说明拟合的越好
注意：$R^2$只能用于拟合函数是线性函数时，拟合结果的评价；如果要比较拟合的好坏，也可以参考SSE



Q：最小二乘法与*极大似然估计*的关系？
