# Transforming Cooling Optimization for Green DataCenter via Deep Reinforcement Learning

### 觉得好的部分
- 技术路线很清晰

### DDPG算法
DDPG（Deep Deterministic Policy Gradient）算法是一种用于连续动作空间的强化学习算法。它结合了深度学习方法和确定性策略梯度方法，通过神经网络来学习一个确定性的策略，该策略能够直接输出在给定状态下应采取的动作。DDPG算法解决了传统DQN算法在连续动作空间中应用的问题，并且采用了经验回放（Experience Replay）和目标网络（Target Network）的技术来提高学习稳定性和效率。

DDPG算法的核心组成部分包括：
1. **Actor网络**：负责生成给定状态下的动作。这个网络是确定性的，输出是具体的动作值，而不是动作的概率分布。
2. **Critic网络**：用于评估当前策略下的状态-动作对的期望回报。Critic网络帮助算法评估Actor所采取的动作的质量。
3. **经验回放**：DDPG使用一个缓冲区来存储过去的经验（状态、动作、奖励、新状态），并在训练过程中随机抽取这些经验来更新Critic网络。
4. **目标网络**：为了稳定训练过程，DDPG为Actor和Critic网络都创建了一个目标网络，这些目标网络的参数会以较慢的速度更新，以减少训练过程中的不稳定性。
5. **探索策略**：DDPG通常使用添加噪声的方法来增加探索性，例如使用Ornstein-Uhlenbeck过程或高斯噪声。

DDPG算法的训练过程涉及到交替更新Critic和Actor网络。首先，Critic网络使用TD-error来评估当前策略下的动作价值，然后Actor网络根据Critic的评价来调整自己的策略以最大化期望回报。这个过程是迭代进行的，直到策略收敛。

DDPG算法是第一个展示深度强化学习可以在连续动作空间中取得成功的算法之一，它在多个复杂的控制任务中取得了突破性的成果。

### Actor-Critic 架构
Actor-Critic 架构是一种在强化学习中常用的算法框架，它包含两个主要的组成部分：Actor 和 Critic。

**Actor** 是指策略网络，其作用是生成策略，即在给定状态下选择一个动作。Actor 的目标是学习一个最优策略，该策略能够最大化累积奖励。

**Critic** 是指值函数网络，它用于评估当前策略的价值，即估算在当前策略下从特定状态出发的期望回报。Critic 通过计算状态-动作对的值函数（Q值）来辅助Actor进行学习。

Actor-Critic 算法的基本思想是利用Critic来评估当前策略的表现，然后Actor根据Critic的反馈来调整自己的策略。这个过程通常涉及策略梯度（Policy Gradient）方法，通过梯度上升来优化策略。

此外，一些Actor-Critic算法还会引入一些高级特性，如经验回放（Experience Replay）、目标网络（Target Network）和温度参数（Temperature Parameter）等，以提高学习效率和稳定性。

在搜索得到的资源中，有几篇论文提到了Actor-Critic架构的不同变种和应用：

- 提到了从Policy Gradient的目标函数出发，可以推导出Actor-Critic架构。
- 解释了Actor-Critic算法的基本组成部分，包括策略网络和评论网络。
- 和讨论了Double Actor-Critic (DAC) 架构，这是一种特殊的Actor-Critic架构，用于学习选项（options）。
- 提出了一个完全分布式的Actor-Critic架构，用于多任务强化学习。
- 提出了一个实时Actor-Critic架构（RTAC），用于应用离线强化学习方法进行连续实时控制。

### TS optimation
TS optimization procedure（过渡态优化程序）是计算化学中用于确定化学反应中过渡态结构的一种计算方法。过渡态是指在化学反应路径上的一个特定点，它是能量最高点，并且沿着反应坐标方向是能量最大点，而在所有其他方向上是能量最小点。这个最高点对应于反应的活化能，是反应动力学研究中的一个重要概念。

过渡态优化的目标是找到一个分子结构，该结构具有一个虚频（即一个为负的二次微分能量，表明体系在该模式下是不稳定的），并且这个结构的能量比反应物和产物的能量都要高。在实际操作中，这通常涉及到以下几个步骤：

1. **初始猜测**：基于反应物和产物的结构，提供一个过渡态的初始猜测。

2. **能量最小化**：在反应坐标方向上进行能量最小化，以确定过渡态结构。

3. **振动频率分析**：对优化后的结构进行振动频率分析，以确认存在一个且仅有一个虚频。

4. **反应路径验证**：通过内禀反应坐标（IRC）计算来验证过渡态，确保它连接反应物和产物，并且沿着反应坐标下降。

5. **可能的迭代**：如果初始过渡态猜测不正确，可能需要调整猜测结构并重复上述步骤。

在搜索结果中，有几篇文档提到了与过渡态优化相关的信息：

- 和讨论了使用Gaussian软件进行过渡态搜索的方法，包括QST2和QST3方法，这些方法使用线性或抛物线同步过境（L/Q-ST）来近似鞍点区域，然后切换到准牛顿搜索以找到真正的过渡态结构。
- 和描述了使用冻结键近似的过渡态猜测方法，其中包括构建最可能的过渡态模型并对不参与反应的所有原子进行优化，然后重新优化过渡态，现在所有原子都是自由的。
- 介绍了3-结构同步过境引导的准牛顿方法（QST3），该方法需要一个“最佳猜测”或模型作为过渡态的输入。
- 讨论了势能面扫描（PES scan），这是一种在指定坐标上计算中间结构能量的方法，可以用来近似反应路径。
- 和提到了CASTEP软件包中的过渡态搜索任务，它允许用户优化过渡态，并预测化学反应的能垒和反应路径。
- 详细描述了CASTEP中使用的几种过渡态搜索算法，包括线性同步过境（LST）方法和二次同步过境（QST）方法。

### Deep Q-network
Deep Q-Network (DQN) 是一种强化学习算法，它使用深度神经网络来逼近动作价值函数，也就是所谓的Q函数。Q函数表示了在给定状态下采取特定动作的期望回报。DQN算法由 DeepMind 的研究人员在 2015 年提出，它在许多具有挑战性的任务中取得了突破性进展，包括经典的雅达利游戏。

DQN 的核心思想是利用深度学习的强大能力来处理高维度的输入数据，并将其应用于强化学习问题。在 DQN 中，深度神经网络被称为 Q-网络，它学习如何预测 Q 函数的值，从而指导智能体（agent）在环境中做出决策。

DQN 的关键组成部分包括：

1. **Q-网络**：一个深度神经网络，输入是环境的状态，输出是对应于每个可能动作的 Q 函数值。

2. **探索与利用**：智能体需要在探索新策略和利用已知信息之间做出平衡。这通常通过 ε-贪心策略来实现，其中 ε 决定了智能体随机选择动作的概率。

3. **经验回放**：DQN 使用一种称为经验回放的技术，它将智能体的经验（即状态、动作、奖励和下一个状态的元组）存储在一个回放缓冲区中。然后，算法可以从这个缓冲区中随机抽取样本来进行学习和目标网络的更新，这有助于提高数据效率和算法的稳定性。

4. **目标网络**：DQN 维护两个 Q-网络，一个是用于预测的在线网络，另一个是用于计算目标值的目标网络。目标网络是在线网络的一个延迟更新副本，它有助于减少学习过程中的噪声。

5. **损失函数**：DQN 使用一种特殊的损失函数，即时间差分误差，来训练 Q-网络。这个损失函数衡量了网络预测的 Q 函数值和真实目标值之间的差异。

DQN 算法的训练过程通常包括以下步骤：

- 智能体执行动作并观察环境的反馈（奖励和下一个状态）。
- 智能体将经验存储在回放缓冲区中。
- 从回放缓冲区中随机抽取一批经验。
- 使用这些经验来更新 Q-网络，通过最小化损失函数来调整网络的权重。

DQN 算法的提出是深度学习和强化学习领域的一个重要里程碑，它证明了深度学习可以与强化学习结合来解决复杂的决策问题。此外，DQN 也为后续的许多先进算法，如 Double DQN、Dueling DQN 和 Rainbow DQN 等，奠定了基础。


