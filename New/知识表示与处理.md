> [!note] 期末考点
> 1. (隐)马尔可夫概率计算

# 绪论

## 知识处理

# 知识获取
## 发展背景
### 知识工程

> [!info] **符号主义**
> - 人工智能源于数理逻辑
> - 智慧的本质是符号的操作和运算
> $$AI\quad System = Knowledge + Reasoning$$

**知识工程**是以知识为处理对象，研究知识系统的知识表示、处理和应用的方法和开发工具的学科
- 规模小
- 成本高
- 知识汤
- 依赖人工构建
- 存在瓶颈，人类大脑知识过多，难以对机器进行编码

深度学习实现事物与对象的识别关系
对实体、实体关系、不同组合表达的事件、推理的规则等进行抽取

#### 知识图谱工程：从文本获取知识
- 命名实体识别
- 术语抽取(概念抽取)：从语料中发现多个单词组成的相关术语
- 关系抽取
- 事件抽取

知识图谱 ≠ 专家系统

### 知识获取基本概念
>[!info] 知识的概念
> Knowledge is <font color='red'> justified true </font>belief. 
> 知识是一种确证了的，真实的信念

**知识获取**(Knoeledge Acquisition, KA)可以被简单地表述为从领域专家的交互中获取知识
基本要求：
	1. 准确性：准确代表领域专家的经验和思维方法
	2. 可靠性：被大多数领域专家所公认和理解，并能经得起实践检验
	3. 完整性：检查或保持已获取知识集合的一致性或无矛盾性
	4. 精炼性：无冗余

**步骤**
	1. 识别领域知识基本结构
	2. 抽取细节知识转换成机器可识别的形式
	3. 调试精炼知识库
#### 知识获取的途径
1. 借助于知识工程师从专家获取
	 1) 无推理能力
	 2) 容易产生认知误差
	 3) 专家资源的缺乏
2. 借助于知识获取程序从专家获取(半自动知识获取)
3. 借助于归纳程序从大量数据中归纳出所需知识
4. 借助于文本理解程序从教科书或科技资料中提炼出所需知识

- 直接知识获取
- 半自动知识获取
- 自动知识获取
	- 知识系统本身具有自学习能力
	- 开发专门的机器学习系统，让机器自动从实际问题中获取知识

## 面向非结构化数据的知识获取
三任务：
	实体抽取、关系抽取与事件抽取

### 实体识别常用方法：基于模板和规则
- 将文本与规则进行匹配来识别出命名实体
	- "xxx说", "xxx老师"...
优点：
- 准确，有些实体识别只能依靠规则抽取
缺点：
- 需要大量的语言学知识
- 需要谨慎处理规则之间的冲突问题
- 构建规则的过程费时费力

### 基于序列标注的方法
使用机器学习算法进行实体识别任务
- 分类器
	- 词本身的特征：边界、依存
	- 前后缀特征
	- 字本身特征
常见序列标注模型：隐马尔可夫模型
有向图模型
基于马尔可夫性，假设特征之间是独立的

>[!info] 马尔可夫链
>考虑一个随机变量的序列 $X = {X_0,X_1,...,X_t}$，这里 $X_t$ 表示时刻 $t$的随机变量，$t=0,1,2,...$。每个随机变量$X_t(t=0,1,2,...)$的取值集合相同，称为状态空间，表示为$S$。随机变量可以是离散的，也可以是连续的.
### 基本定义
直观解释：“未来只依赖于现在，而与过去无关”

n 阶马尔可夫链：$$P(X_t|X_0 X_1....X_{t-2}X_{t-1}) = P(X_t|X{t-n}...X_{t-1})$$
离散状态马尔可夫链 $X=\{X_0,X_1,...,X_t,...\}$，随机变量

转移概率$p_{ij}$可以由矩阵表示，即$$$$
初始分布 $\pi(0)$ 一般只有一个位置是1，其余均为0

马尔可夫链X在时刻 $t$ 的状态分布，可以由在时刻 $(t-1)$ 的状态分布以及转移概率分布决定$$\pi(t)=P\pi(t-1)$$
马尔可夫链在时刻 $t$ 的状态分布，可以通过递推得到。由$$\pi(t)=P\pi(t-1)=P(P\pi(t-2))=P^2\pi(t-2)$$
这里的$P^t$称为 $t$ 步转移概率矩阵

### 隐马尔科夫模型
> [!info] 介绍
> 由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测从而产生观测随机序列的过程。
> 隐藏的马尔可夫链随机生成的状态的序列，成为状态序列；每个状态生成一个观测，由此产生的观测的随机序列，称为观测序列，序列的每一个位置可看作是一个时刻

**基本假设：**
- 齐次马尔可夫性假设：隐马尔可夫链 $t$ 的状态只和 $t-1$ 状态有关
- 观测独立性假设：观测只和当前时刻状态有关

**观测序列的生成：**
- 输入：隐马尔可夫模型$\lambda=(A,B, \pi)$，观测序列长度$T$
- 输出：观测序列 $O=(o_1, o_2,...,o_T)$
1) 按照初始状态分布 $\pi$ 生成状态 $i_1$
2) 令 $t=1$
3) 按照状态 $i_t$的观测概率分布$b_{i_t}(k)$生成$o_t$
4) 按照状态$i_t$的状态转移概率分布$\{o_{i,i_{t+1}}\}$产生状态$i_{t+1}$，$i_{t+1}=1,2,...,N$
5) 令 $t=t+1$，如果$t<T$，转步(3)，否则终止

**三个基本问题：**
1) 概率计算问题
	给定模型$\lambda=\{A, B, \pi\}$和观测序列 $O=\{o_1, o_2, ..., o_T\}$，计算在模型 $\lambda$ 下观测序列 O出现的概率$P(O|\lambda)$
	 
1) 学习问题
	 已知观测序列，估计模型参数，使得在该模型下观测序列概率最大，即用极大似然的方法估计参数
3) 预测问题
	 也称为解码问题。已知模型和观测序列，求对给定
